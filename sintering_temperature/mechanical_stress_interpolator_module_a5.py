import streamlit as st
import numpy as np
from numba import jit, prange
import matplotlib.pyplot as plt
import pandas as pd
import zipfile
from io import BytesIO
import time
import hashlib
import json
from datetime import datetime
import warnings
import pickle
import torch
import sqlite3
from pathlib import Path
import tempfile
import os
import glob
from typing import List, Dict, Any, Optional, Tuple, Union
from itertools import product
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
warnings.filterwarnings('ignore')
# =============================================
# PATH CONFIGURATION
# =============================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
NUMERICAL_SOLUTIONS_DIR = os.path.join(SCRIPT_DIR, "numerical_solutions")
if not os.path.exists(NUMERICAL_SOLUTIONS_DIR):
    os.makedirs(NUMERICAL_SOLUTIONS_DIR, exist_ok=True)
# =============================================
# PREDICTION RESULTS SAVING AND DOWNLOAD MANAGER
# =============================================
class PredictionResultsManager:
    """Manager for saving and downloading prediction results"""
 
    @staticmethod
    def prepare_prediction_data_for_saving(prediction_results: Dict[str, Any],
                                         source_simulations: List[Dict],
                                         mode: str = 'single') -> Dict[str, Any]:
        """
        Prepare prediction results for saving to file
     
        Args:
            prediction_results: Dictionary of prediction results
            source_simulations: List of source simulation data
            mode: 'single' for single target, 'multi' for multiple targets
         
        Returns:
            Structured dictionary ready for saving
        """
        # Create metadata
        metadata = {
            'save_timestamp': datetime.now().isoformat(),
            'mode': mode,
            'num_sources': len(source_simulations),
            'software_version': '1.0.0',
            'data_type': 'attention_interpolation_results'
        }
     
        # Extract source parameters
        source_params = []
        for i, sim_data in enumerate(source_simulations):
            params = sim_data.get('params', {})
            source_params.append({
                'id': i,
                'defect_type': params.get('defect_type'),
                'shape': params.get('shape'),
                'orientation': params.get('orientation'),
                'eps0': float(params.get('eps0', 0)),
                'kappa': float(params.get('kappa', 0)),
                'theta': float(params.get('theta', 0))
            })
     
        # Structure the data
        save_data = {
            'metadata': metadata,
            'source_parameters': source_params,
            'prediction_results': prediction_results.copy() # Create a copy to avoid modifications
        }
     
        # Add additional info based on mode
        if mode == 'single' and 'attention_weights' in prediction_results:
            weights = prediction_results['attention_weights']
            save_data['attention_analysis'] = {
                'weights': weights.tolist() if hasattr(weights, 'tolist') else weights,
                'source_names': [f'S{i+1}' for i in range(len(source_simulations))],
                'dominant_source': int(np.argmax(weights)) if hasattr(weights, '__len__') else 0,
                'weight_entropy': float(-np.sum(weights * np.log(weights + 1e-10)))
            }
     
        # Add stress statistics if available
        if 'stress_fields' in prediction_results:
            stress_stats = {}
            for field_name, field_data in prediction_results['stress_fields'].items():
                if isinstance(field_data, np.ndarray):
                    stress_stats[field_name] = {
                        'max': float(np.max(field_data)),
                        'min': float(np.min(field_data)),
                        'mean': float(np.mean(field_data)),
                        'std': float(np.std(field_data)),
                        'percentile_95': float(np.percentile(field_data, 95)),
                        'percentile_99': float(np.percentile(field_data, 99))
                    }
            save_data['stress_statistics'] = stress_stats
     
        return save_data
 
    @staticmethod
    def create_single_prediction_archive(prediction_results: Dict[str, Any],
                                       source_simulations: List[Dict]) -> BytesIO:
        """
        Create a comprehensive archive for single prediction
     
        Args:
            prediction_results: Single prediction results
            source_simulations: List of source simulations
         
        Returns:
            BytesIO buffer containing the archive
        """
        # Create in-memory zip file
        zip_buffer = BytesIO()
     
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # 1. Save main prediction data as PKL
            save_data = PredictionResultsManager.prepare_prediction_data_for_saving(
                prediction_results, source_simulations, 'single'
            )
         
            # PKL format
            pkl_data = pickle.dumps(save_data, protocol=pickle.HIGHEST_PROTOCOL)
            zip_file.writestr('prediction_results.pkl', pkl_data)
         
            # 2. Save as PT (PyTorch) format
            pt_buffer = BytesIO()
            torch.save(save_data, pt_buffer)
            pt_buffer.seek(0)
            zip_file.writestr('prediction_results.pt', pt_buffer.read())
         
            # 3. Save stress fields as separate NPZ files
            stress_fields = prediction_results.get('stress_fields', {})
            for field_name, field_data in stress_fields.items():
                if isinstance(field_data, np.ndarray):
                    npz_buffer = BytesIO()
                    np.savez_compressed(npz_buffer, data=field_data)
                    npz_buffer.seek(0)
                    zip_file.writestr(f'stress_{field_name}.npz', npz_buffer.read())
         
            # 4. Save attention weights as CSV
            if 'attention_weights' in prediction_results:
                weights = prediction_results['attention_weights']
                if hasattr(weights, 'flatten'):
                    weights = weights.flatten()
             
                weight_df = pd.DataFrame({
                    'source_id': [f'S{i+1}' for i in range(len(weights))],
                    'weight': weights,
                    'percent_contribution': 100 * weights / (np.sum(weights) + 1e-10)
                })
                csv_data = weight_df.to_csv(index=False)
                zip_file.writestr('attention_weights.csv', csv_data)
         
            # 5. Save target parameters as JSON
            target_params = prediction_results.get('target_params', {})
            if target_params:
                # Convert numpy types to Python types for JSON
                def convert_for_json(obj):
                    if isinstance(obj, (np.float32, np.float64, np.float16)):
                        return float(obj)
                    elif isinstance(obj, (np.int32, np.int64, np.int16, np.int8)):
                        return int(obj)
                    elif isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, np.generic):
                        return obj.item()
                    else:
                        return obj
             
                json_data = json.dumps(target_params, default=convert_for_json, indent=2)
                zip_file.writestr('target_parameters.json', json_data)
         
            # 6. Save summary statistics
            if 'stress_fields' in prediction_results:
                stats_rows = []
                for field_name, field_data in stress_fields.items():
                    if isinstance(field_data, np.ndarray):
                        stats_rows.append({
                            'field': field_name,
                            'max': float(np.max(field_data)),
                            'min': float(np.min(field_data)),
                            'mean': float(np.mean(field_data)),
                            'std': float(np.std(field_data)),
                            'percentile_95': float(np.percentile(field_data, 95)),
                            'percentile_99': float(np.percentile(field_data, 99)),
                            'area_above_threshold': float(np.sum(field_data > np.mean(field_data)))
                        })
             
                if stats_rows:
                    stats_df = pd.DataFrame(stats_rows)
                    stats_csv = stats_df.to_csv(index=False)
                    zip_file.writestr('stress_statistics.csv', stats_csv)
         
            # 7. Save a README file
            readme_content = f"""# Prediction Results Archive
Generated: {datetime.now().isoformat()}
Number of source simulations: {len(source_simulations)}
Prediction mode: Single target
Files included:
1. prediction_results.pkl - Main prediction data (Python pickle format)
2. prediction_results.pt - PyTorch format
3. stress_*.npz - Individual stress fields (NumPy compressed)
4. attention_weights.csv - Attention weights distribution
5. target_parameters.json - Target parameters
6. stress_statistics.csv - Statistical summary
For more information, see the documentation.
"""
            zip_file.writestr('README.txt', readme_content)
     
        zip_buffer.seek(0)
        return zip_buffer
 
    @staticmethod
    def create_multi_prediction_archive(multi_predictions: Dict[str, Any],
                                       source_simulations: List[Dict]) -> BytesIO:
        """
        Create a comprehensive archive for multiple predictions
     
        Args:
            multi_predictions: Dictionary of multiple predictions
            source_simulations: List of source simulations
         
        Returns:
            BytesIO buffer containing the archive
        """
        zip_buffer = BytesIO()
     
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # Save each prediction individually
            for pred_key, pred_data in multi_predictions.items():
                # Create directory for each prediction
                pred_dir = f'predictions/{pred_key}'
             
                # Save prediction data
                save_data = PredictionResultsManager.prepare_prediction_data_for_saving(
                    pred_data, source_simulations, 'multi'
                )
             
                # Save as PKL
                pkl_data = pickle.dumps(save_data, protocol=pickle.HIGHEST_PROTOCOL)
                zip_file.writestr(f'{pred_dir}/prediction.pkl', pkl_data)
             
                # Save stress statistics
                stress_fields = {k: v for k, v in pred_data.items()
                               if isinstance(v, np.ndarray) and k in ['sigma_hydro', 'sigma_mag', 'von_mises']}
             
                if stress_fields:
                    stats_rows = []
                    for field_name, field_data in stress_fields.items():
                        stats_rows.append({
                            'field': field_name,
                            'max': float(np.max(field_data)),
                            'min': float(np.min(field_data)),
                            'mean': float(np.mean(field_data)),
                            'std': float(np.std(field_data))
                        })
                 
                    stats_df = pd.DataFrame(stats_rows)
                    stats_csv = stats_df.to_csv(index=False)
                    zip_file.writestr(f'{pred_dir}/statistics.csv', stats_csv)
         
            # Save master summary
            summary_rows = []
            for pred_key, pred_data in multi_predictions.items():
                target_params = pred_data.get('target_params', {})
                stress_fields = {k: v for k, v in pred_data.items()
                               if isinstance(v, np.ndarray) and k in ['sigma_hydro', 'sigma_mag', 'von_mises']}
             
                row = {
                    'prediction_id': pred_key,
                    'defect_type': target_params.get('defect_type', 'Unknown'),
                    'shape': target_params.get('shape', 'Unknown'),
                    'orientation': target_params.get('orientation', 'Unknown'),
                    'eps0': float(target_params.get('eps0', 0)),
                    'kappa': float(target_params.get('kappa', 0)),
                    'theta_deg': float(np.rad2deg(target_params.get('theta', 0)))
                }
             
                # Add stress metrics
                for field_name, field_data in stress_fields.items():
                    row[f'{field_name}_max'] = float(np.max(field_data))
                    row[f'{field_name}_mean'] = float(np.mean(field_data))
                    row[f'{field_name}_std'] = float(np.std(field_data))
             
                summary_rows.append(row)
         
            if summary_rows:
                summary_df = pd.DataFrame(summary_rows)
                summary_csv = summary_df.to_csv(index=False)
                zip_file.writestr('multi_prediction_summary.csv', summary_csv)
         
            # Save a README file
            readme_content = f"""# Multi-Prediction Results Archive
Generated: {datetime.now().isoformat()}
Number of source simulations: {len(source_simulations)}
Number of predictions: {len(multi_predictions)}
Structure:
- predictions/[prediction_id]/ - Individual prediction data
- multi_prediction_summary.csv - Summary of all predictions
Each prediction directory contains:
1. prediction.pkl - Main prediction data
2. statistics.csv - Stress statistics
For more information, see the documentation.
"""
            zip_file.writestr('README.txt', readme_content)
     
        zip_buffer.seek(0)
        return zip_buffer
# =============================================
# NUMERICAL SOLUTIONS MANAGER (UPDATED FOR NUMERICAL_SOLUTIONS)
# =============================================
class NumericalSolutionsManager:
    def __init__(self, solutions_dir: str = NUMERICAL_SOLUTIONS_DIR):
        self.solutions_dir = solutions_dir
        self._ensure_directory()
 
    def _ensure_directory(self):
        if not os.path.exists(self.solutions_dir):
            os.makedirs(self.solutions_dir, exist_ok=True)
 
    def scan_directory(self) -> Dict[str, List[str]]:
        file_formats = {
            'pkl': [],
            'pt': [],
            'h5': [],
            'npz': [],
            'sql': [],
            'json': []
        }
     
        for format_type, extensions in [
            ('pkl', ['*.pkl', '*.pickle']),
            ('pt', ['*.pt', '*.pth']),
            ('h5', ['*.h5', '*.hdf5']),
            ('npz', ['*.npz']),
            ('sql', ['*.sql', '*.db']),
            ('json', ['*.json'])
        ]:
            for ext in extensions:
                pattern = os.path.join(self.solutions_dir, ext)
                files = glob.glob(pattern)
                if files:
                    files.sort(key=os.path.getmtime, reverse=True)
                    file_formats[format_type].extend(files)
     
        return file_formats
 
    def get_all_files(self) -> List[Dict[str, Any]]:
        all_files = []
        file_formats = self.scan_directory()
     
        for format_type, files in file_formats.items():
            for file_path in files:
                file_info = {
                    'path': file_path,
                    'filename': os.path.basename(file_path),
                    'format': format_type,
                    'size': os.path.getsize(file_path),
                    'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                    'relative_path': os.path.relpath(file_path, self.solutions_dir)
                }
                all_files.append(file_info)
     
        all_files.sort(key=lambda x: x['filename'].lower())
        return all_files
 
    def get_file_by_name(self, filename: str) -> Optional[str]:
        for file_info in self.get_all_files():
            if file_info['filename'] == filename:
                return file_info['path']
        return None
 
    def load_simulation(self, file_path: str, interpolator) -> Dict[str, Any]:
        try:
            ext = os.path.splitext(file_path)[1].lower().lstrip('.')
            if ext in ['pkl', 'pickle']:
                format_type = 'pkl'
            elif ext in ['pt', 'pth']:
                format_type = 'pt'
            elif ext in ['h5', 'hdf5']:
                format_type = 'h5'
            elif ext == 'npz':
                format_type = 'npz'
            elif ext in ['sql', 'db']:
                format_type = 'sql'
            elif ext == 'json':
                format_type = 'json'
            else:
                format_type = 'auto'
         
            with open(file_path, 'rb') as f:
                file_content = f.read()
         
            sim_data = interpolator.read_simulation_file(file_content, format_type)
            sim_data['loaded_from'] = 'numerical_solutions'
            return sim_data
         
        except Exception as e:
            st.error(f"Error loading {file_path}: {str(e)}")
            raise
 
    def save_simulation(self, data: Dict[str, Any], filename: str, format_type: str = 'pkl'):
        if not filename.endswith(f'.{format_type}'):
            filename = f"{filename}.{format_type}"
     
        file_path = os.path.join(self.solutions_dir, filename)
     
        try:
            if format_type == 'pkl':
                with open(file_path, 'wb') as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
         
            elif format_type == 'pt':
                torch.save(data, file_path)
         
            elif format_type == 'json':
                def convert_for_json(obj):
                    if isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, np.generic):
                        return obj.item()
                    elif isinstance(obj, dict):
                        return {k: convert_for_json(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_for_json(item) for item in obj]
                    else:
                        return obj
             
                json_data = convert_for_json(data)
                with open(file_path, 'w') as f:
                    json.dump(json_data, f, indent=2)
         
            else:
                st.warning(f"Format {format_type} not supported for saving")
                return False
         
            st.success(f"‚úÖ Saved simulation to: {filename}")
            return True
         
        except Exception as e:
            st.error(f"Error saving file: {str(e)}")
            return False
# =============================================
# MULTI-TARGET PREDICTION MANAGER
# =============================================
class MultiTargetPredictionManager:
    """Manager for handling multiple target predictions"""
 
    @staticmethod
    def create_parameter_grid(base_params, ranges_config):
        """
        Create a grid of parameter combinations based on ranges
     
        Args:
            base_params: Base parameter dictionary
            ranges_config: Dictionary with range specifications
                Example: {
                    'eps0': {'min': 0.5, 'max': 2.0, 'steps': 10},
                    'kappa': {'min': 0.2, 'max': 1.0, 'steps': 5},
                    'theta': {'values': [0, np.pi/6, np.pi/3, np.pi/2]}
                }
     
        Returns:
            List of parameter dictionaries
        """
        param_grid = []
     
        # Prepare parameter value lists
        param_values = {}
     
        for param_name, config in ranges_config.items():
            if 'values' in config:
                # Specific values provided
                param_values[param_name] = config['values']
            elif 'min' in config and 'max' in config:
                # Range with steps
                steps = config.get('steps', 10)
                param_values[param_name] = np.linspace(
                    config['min'], config['max'], steps
                ).tolist()
            else:
                # Single value
                param_values[param_name] = [config.get('value', base_params.get(param_name))]
     
        # Generate all combinations
        param_names = list(param_values.keys())
        value_arrays = [param_values[name] for name in param_names]
     
        for combination in product(*value_arrays):
            param_dict = base_params.copy()
            for name, value in zip(param_names, combination):
                param_dict[name] = float(value) if isinstance(value, (int, float, np.number)) else value
         
            param_grid.append(param_dict)
     
        return param_grid
 
    @staticmethod
    def batch_predict(source_simulations, target_params_list, interpolator):
        """
        Perform batch predictions for multiple targets
     
        Args:
            source_simulations: List of source simulation data
            target_params_list: List of target parameter dictionaries
            interpolator: SpatialLocalityAttentionInterpolator instance
     
        Returns:
            Dictionary with predictions for each target
        """
        predictions = {}
     
        # Prepare source data
        source_param_vectors = []
        source_stress_data = []
     
        for sim_data in source_simulations:
            param_vector, _ = interpolator.compute_parameter_vector(sim_data)
            source_param_vectors.append(param_vector)
         
            # Get stress from final frame
            history = sim_data.get('history', [])
            if history:
                eta, stress_fields = history[-1]
                stress_components = np.stack([
                    stress_fields.get('sigma_hydro', np.zeros_like(eta)),
                    stress_fields.get('sigma_mag', np.zeros_like(eta)),
                    stress_fields.get('von_mises', np.zeros_like(eta))
                ], axis=0)
                source_stress_data.append(stress_components)
     
        source_param_vectors = np.array(source_param_vectors)
        source_stress_data = np.array(source_stress_data)
     
        # Predict for each target
        for idx, target_params in enumerate(target_params_list):
            # Compute target parameter vector
            target_vector, _ = interpolator.compute_parameter_vector(
                {'params': target_params}
            )
         
            # Calculate distances and weights
            distances = np.sqrt(np.sum((source_param_vectors - target_vector) ** 2, axis=1))
            weights = np.exp(-0.5 * (distances / 0.3) ** 2)
            weights = weights / (np.sum(weights) + 1e-8)
         
            # Weighted combination
            weighted_stress = np.sum(
                source_stress_data * weights[:, np.newaxis, np.newaxis, np.newaxis],
                axis=0
            )
         
            predicted_stress = {
                'sigma_hydro': weighted_stress[0],
                'sigma_mag': weighted_stress[1],
                'von_mises': weighted_stress[2],
                'predicted': True,
                'target_params': target_params,
                'attention_weights': weights,
                'target_index': idx
            }
         
            predictions[f"target_{idx:03d}"] = predicted_stress
     
        return predictions
# =============================================
# ENHANCED SPATIAL LOCALITY REGULARIZATION ATTENTION INTERPOLATOR
# =============================================
class SpatialLocalityAttentionInterpolator:
    """Enhanced attention-based interpolator with spatial locality regularization"""
 
    def __init__(self, input_dim=15, num_heads=4, d_model=32, output_dim=3,
                 sigma_spatial=0.2, sigma_param=0.2, use_gaussian=True):
        self.input_dim = input_dim
        self.num_heads = num_heads
        self.d_model = d_model
        self.output_dim = output_dim
        self.sigma_spatial = sigma_spatial
        self.sigma_param = sigma_param
        self.use_gaussian = use_gaussian
     
        self.model = self._build_model()
     
        self.readers = {
            'pkl': self._read_pkl,
            'pt': self._read_pt,
            'h5': self._read_h5,
            'npz': self._read_npz,
            'sql': self._read_sql,
            'json': self._read_json
        }
 
    def _build_model(self):
        model = torch.nn.ModuleDict({
            'param_embedding': torch.nn.Sequential(
                torch.nn.Linear(self.input_dim, self.d_model * 2),
                torch.nn.ReLU(),
                torch.nn.Linear(self.d_model * 2, self.d_model)
            ),
            'attention': torch.nn.MultiheadAttention(
                embed_dim=self.d_model,
                num_heads=self.num_heads,
                batch_first=True,
                dropout=0.1
            ),
            'feed_forward': torch.nn.Sequential(
                torch.nn.Linear(self.d_model, self.d_model * 4),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.1),
                torch.nn.Linear(self.d_model * 4, self.d_model)
            ),
            'output_projection': torch.nn.Sequential(
                torch.nn.Linear(self.d_model, self.d_model * 2),
                torch.nn.ReLU(),
                torch.nn.Linear(self.d_model * 2, self.output_dim)
            ),
            'spatial_regularizer': torch.nn.Sequential(
                torch.nn.Linear(2, 32),
                torch.nn.ReLU(),
                torch.nn.Linear(32, self.num_heads)
            ) if self.use_gaussian else None,
            'norm1': torch.nn.LayerNorm(self.d_model),
            'norm2': torch.nn.LayerNorm(self.d_model)
        })
        return model
 
    def _read_pkl(self, file_content):
        buffer = BytesIO(file_content)
        return pickle.load(buffer)
 
    def _read_pt(self, file_content):
        buffer = BytesIO(file_content)
        return torch.load(buffer, map_location=torch.device('cpu'))
 
    def _read_h5(self, file_content):
        import h5py
        buffer = BytesIO(file_content)
        with h5py.File(buffer, 'r') as f:
            data = {}
            def read_h5_obj(name, obj):
                if isinstance(obj, h5py.Dataset):
                    data[name] = obj[()]
                elif isinstance(obj, h5py.Group):
                    data[name] = {}
                    for key in obj.keys():
                        read_h5_obj(f"{name}/{key}", obj[key])
            for key in f.keys():
                read_h5_obj(key, f[key])
        return data
 
    def _read_npz(self, file_content):
        buffer = BytesIO(file_content)
        data = np.load(buffer, allow_pickle=True)
        return {key: data[key] for key in data.files}
 
    def _read_sql(self, file_content):
        with tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False) as tmp:
            tmp.write(file_content)
            tmp_path = tmp.name
     
        try:
            conn = sqlite3.connect(tmp_path)
            cursor = conn.cursor()
         
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
         
            data = {}
            for table in tables:
                table_name = table[0]
                cursor.execute(f"SELECT * FROM {table_name}")
                columns = [description[0] for description in cursor.description]
                rows = cursor.fetchall()
                data[table_name] = {
                    'columns': columns,
                    'rows': rows
                }
         
            conn.close()
            os.unlink(tmp_path)
            return data
        except Exception as e:
            os.unlink(tmp_path)
            raise e
 
    def _read_json(self, file_content):
        return json.loads(file_content.decode('utf-8'))
 
    def read_simulation_file(self, file_content, format_type='auto'):
        if format_type == 'auto':
            format_type = 'pkl'
     
        if format_type in self.readers:
            data = self.readers[format_type](file_content)
            return self._standardize_data(data, format_type, "uploaded_file")
        else:
            raise ValueError(f"Unsupported format: {format_type}")
 
    def _standardize_data(self, data, format_type, file_path):
        standardized = {
            'params': {},
            'history': [],
            'metadata': {},
            'format': format_type,
            'file_path': file_path,
            'filename': os.path.basename(file_path) if isinstance(file_path, str) else "uploaded"
        }
     
        if format_type == 'pkl':
            if isinstance(data, dict):
                standardized['params'] = data.get('params', {})
                standardized['metadata'] = data.get('metadata', {})
             
                for frame in data.get('history', []):
                    if isinstance(frame, dict):
                        eta = frame.get('eta')
                        stresses = frame.get('stresses', {})
                        standardized['history'].append((eta, stresses))
     
        elif format_type == 'pt':
            if isinstance(data, dict):
                standardized['params'] = data.get('params', {})
                standardized['metadata'] = data.get('metadata', {})
             
                for frame in data.get('history', []):
                    if isinstance(frame, dict):
                        eta = frame.get('eta')
                        stresses = frame.get('stresses', {})
                     
                        if torch.is_tensor(eta):
                            eta = eta.numpy()
                     
                        stress_dict = {}
                        for key, value in stresses.items():
                            if torch.is_tensor(value):
                                stress_dict[key] = value.numpy()
                            else:
                                stress_dict[key] = value
                     
                        standardized['history'].append((eta, stress_dict))
     
        elif format_type == 'h5':
            if 'params' in data:
                standardized['params'] = data['params']
            if 'metadata' in data:
                standardized['metadata'] = data['metadata']
            for key in data.keys():
                if 'history' in key.lower():
                    standardized['history'] = data[key]
                    break
     
        elif format_type == 'npz':
            if 'params' in data:
                standardized['params'] = data['params']
            if 'metadata' in data:
                standardized['metadata'] = data['metadata']
            if 'history' in data:
                standardized['history'] = data['history']
     
        elif format_type == 'json':
            if isinstance(data, dict):
                standardized['params'] = data.get('params', {})
                standardized['metadata'] = data.get('metadata', {})
                standardized['history'] = data.get('history', [])
     
        return standardized
 
    def compute_parameter_vector(self, sim_data):
        params = sim_data.get('params', {})
     
        param_vector = []
        param_names = []
     
        defect_encoding = {
            'ISF': [1, 0, 0],
            'ESF': [0, 1, 0],
            'Twin': [0, 0, 1]
        }
        defect_type = params.get('defect_type', 'ISF')
        param_vector.extend(defect_encoding.get(defect_type, [0, 0, 0]))
        param_names.extend(['defect_ISF', 'defect_ESF', 'defect_Twin'])
     
        shape_encoding = {
            'Square': [1, 0, 0, 0, 0],
            'Horizontal Fault': [0, 1, 0, 0, 0],
            'Vertical Fault': [0, 0, 1, 0, 0],
            'Rectangle': [0, 0, 0, 1, 0],
            'Ellipse': [0, 0, 0, 0, 1]
        }
        shape = params.get('shape', 'Square')
        param_vector.extend(shape_encoding.get(shape, [0, 0, 0, 0, 0]))
        param_names.extend(['shape_square', 'shape_horizontal', 'shape_vertical',
                           'shape_rectangle', 'shape_ellipse'])
     
        eps0 = params.get('eps0', 0.707)
        kappa = params.get('kappa', 0.6)
        theta = params.get('theta', 0.0)
     
        eps0_norm = (eps0 - 0.3) / (3.0 - 0.3) if eps0 is not None else 0.5
        param_vector.append(eps0_norm)
        param_names.append('eps0_norm')
     
        kappa_norm = (kappa - 0.1) / (2.0 - 0.1) if kappa is not None else 0.5
        param_vector.append(kappa_norm)
        param_names.append('kappa_norm')
     
        theta_norm = (theta % (2 * np.pi)) / (2 * np.pi) if theta is not None else 0.0
        param_vector.append(theta_norm)
        param_names.append('theta_norm')
     
        orientation = params.get('orientation', 'Horizontal {111} (0¬∞)')
        orientation_encoding = {
            'Horizontal {111} (0¬∞)': [1, 0, 0, 0],
            'Tilted 30¬∞ (1¬Ø10 projection)': [0, 1, 0, 0],
            'Tilted 60¬∞': [0, 0, 1, 0],
            'Vertical {111} (90¬∞)': [0, 0, 0, 1]
        }
     
        if orientation.startswith('Custom ('):
            param_vector.extend([0, 0, 0, 0])
        else:
            param_vector.extend(orientation_encoding.get(orientation, [0, 0, 0, 0]))
         
        param_names.extend(['orient_0deg', 'orient_30deg', 'orient_60deg', 'orient_90deg'])
     
        return np.array(param_vector, dtype=np.float32), param_names
 
    @staticmethod
    def get_orientation_from_angle(angle_deg: float) -> str:
        """Convert angle in degrees to orientation string with custom support"""
        if 0 <= angle_deg <= 15:
            return 'Horizontal {111} (0¬∞)'
        elif 15 < angle_deg <= 45:
            return 'Tilted 30¬∞ (1¬Ø10 projection)'
        elif 45 < angle_deg <= 75:
            return 'Tilted 60¬∞'
        elif 75 < angle_deg <= 90:
            return 'Vertical {111} (90¬∞)'
        else:
            angle_deg = angle_deg % 90
            return f"Custom ({angle_deg:.1f}¬∞)"
# =============================================
# GRID AND EXTENT CONFIGURATION
# =============================================
def get_grid_extent(N=128, dx=0.1):
    """Get grid extent for visualization"""
    return [-N*dx/2, N*dx/2, -N*dx/2, N*dx/2]
# =============================================
# ATTENTION INTERFACE
# =============================================
def create_attention_interface():
    """Create the attention interpolation interface with save/download"""
 
    st.header("ü§ñ Spatial-Attention Stress Interpolation")
 
    # Initialize interpolator in session state
    if 'interpolator' not in st.session_state:
        st.session_state.interpolator = SpatialLocalityAttentionInterpolator()
 
    # Initialize numerical solutions manager
    if 'solutions_manager' not in st.session_state:
        st.session_state.solutions_manager = NumericalSolutionsManager()
 
    # Initialize multi-target manager
    if 'multi_target_manager' not in st.session_state:
        st.session_state.multi_target_manager = MultiTargetPredictionManager()
 
    # Initialize prediction results manager
    if 'prediction_results_manager' not in st.session_state:
        st.session_state.prediction_results_manager = PredictionResultsManager()
 
    # Initialize source simulations list
    if 'source_simulations' not in st.session_state:
        st.session_state.source_simulations = []
        st.session_state.uploaded_files = {}
        st.session_state.loaded_from_numerical = []
 
    # Initialize multi-target predictions
    if 'multi_target_predictions' not in st.session_state:
        st.session_state.multi_target_predictions = {}
        st.session_state.multi_target_params = []
 
    # Initialize saving options
    if 'save_format' not in st.session_state:
        st.session_state.save_format = 'both'
  
    # Initialize download data in session state
    if 'download_pkl_data' not in st.session_state:
        st.session_state.download_pkl_data = None
    if 'download_pt_data' not in st.session_state:
        st.session_state.download_pt_data = None
    if 'download_zip_data' not in st.session_state:
        st.session_state.download_zip_data = None
    if 'download_zip_filename' not in st.session_state:
        st.session_state.download_zip_filename = None
 
    # Get grid extent for visualization
    extent = get_grid_extent()
 
    # Sidebar configuration
    st.sidebar.header("üîÆ Attention Interpolator Settings")
 
    with st.sidebar.expander("‚öôÔ∏è Model Parameters", expanded=False):
        num_heads = st.slider("Number of Attention Heads", 1, 8, 4, 1)
        sigma_spatial = st.slider("Spatial Sigma (œÉ_spatial)", 0.05, 1.0, 0.2, 0.05)
        sigma_param = st.slider("Parameter Sigma (œÉ_param)", 0.05, 1.0, 0.3, 0.05)
        use_gaussian = st.checkbox("Use Gaussian Spatial Regularization", True)
     
        if st.button("üîÑ Update Model Parameters"):
            st.session_state.interpolator = SpatialLocalityAttentionInterpolator(
                num_heads=num_heads,
                sigma_spatial=sigma_spatial,
                sigma_param=sigma_param,
                use_gaussian=use_gaussian
            )
            st.success("Model parameters updated!")
 
    with st.sidebar.expander("üíæ Download Options", expanded=True):
        st.session_state.save_format = st.radio(
            "Download Format",
            ["PKL only", "PT only", "Both PKL & PT", "Archive (ZIP)"],
            index=2,
            key="save_format_radio"
        )
 
    # Main interface tabs
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "üì§ Load Source Data",
        "üéØ Configure Target",
        "üéØ Configure Multiple Targets",
        "üöÄ Train & Predict",
        "üìä Results & Visualization",
        "üíæ Export Results"
    ])
 
    # Tab 1: Load Source Data
    with tab1:
        st.subheader("Load Source Simulation Files")
     
        col1, col2 = st.columns([1, 1])
     
        with col1:
            st.markdown("### üìÇ From numerical_solutions Directory")
            st.info(f"Loading from: `{NUMERICAL_SOLUTIONS_DIR}`")
         
            file_formats = st.session_state.solutions_manager.scan_directory()
            all_files_info = st.session_state.solutions_manager.get_all_files()
         
            if not all_files_info:
                st.warning(f"No simulation files found in `{NUMERICAL_SOLUTIONS_DIR}`")
            else:
                file_groups = {}
                for file_info in all_files_info:
                    format_type = file_info['format']
                    if format_type not in file_groups:
                        file_groups[format_type] = []
                    file_groups[format_type].append(file_info)
             
                for format_type, files in file_groups.items():
                    with st.expander(f"{format_type.upper()} Files ({len(files)})", expanded=True):
                        file_options = {}
                        for file_info in files:
                            display_name = f"{file_info['filename']} ({file_info['size'] // 1024}KB)"
                            file_options[display_name] = file_info['path']
                     
                        selected_files = st.multiselect(
                            f"Select {format_type} files",
                            options=list(file_options.keys()),
                            key=f"select_{format_type}"
                        )
                     
                        if selected_files:
                            if st.button(f"üì• Load Selected {format_type} Files", key=f"load_{format_type}"):
                                with st.spinner(f"Loading {len(selected_files)} files..."):
                                    loaded_count = 0
                                    for display_name in selected_files:
                                        file_path = file_options[display_name]
                                        try:
                                            sim_data = st.session_state.solutions_manager.load_simulation(
                                                file_path,
                                                st.session_state.interpolator
                                            )
                                         
                                            if file_path not in st.session_state.loaded_from_numerical:
                                                st.session_state.source_simulations.append(sim_data)
                                                st.session_state.loaded_from_numerical.append(file_path)
                                                loaded_count += 1
                                                st.success(f"‚úÖ Loaded: {os.path.basename(file_path)}")
                                            else:
                                                st.warning(f"‚ö†Ô∏è Already loaded: {os.path.basename(file_path)}")
                                             
                                        except Exception as e:
                                            st.error(f"‚ùå Error loading {os.path.basename(file_path)}: {str(e)}")
                                 
                                    if loaded_count > 0:
                                        st.success(f"Successfully loaded {loaded_count} new files!")
                                        st.rerun()
     
        with col2:
            st.markdown("### üì§ Upload Local Files")
         
            uploaded_files = st.file_uploader(
                "Upload simulation files (PKL, PT, H5, NPZ, SQL, JSON)",
                type=['pkl', 'pt', 'h5', 'hdf5', 'npz', 'sql', 'db', 'json'],
                accept_multiple_files=True
            )
         
            format_type = st.selectbox(
                "File Format (for upload)",
                ["Auto Detect", "PKL", "PT", "H5", "NPZ", "SQL", "JSON"],
                index=0
            )
         
            if uploaded_files and st.button("üì• Load Uploaded Files", type="primary"):
                with st.spinner("Loading uploaded files..."):
                    loaded_sims = []
                    for uploaded_file in uploaded_files:
                        try:
                            file_content = uploaded_file.getvalue()
                            actual_format = format_type.lower() if format_type != "Auto Detect" else "auto"
                            if actual_format == "auto":
                                filename = uploaded_file.name.lower()
                                if filename.endswith('.pkl'):
                                    actual_format = 'pkl'
                                elif filename.endswith('.pt'):
                                    actual_format = 'pt'
                                elif filename.endswith('.h5') or filename.endswith('.hdf5'):
                                    actual_format = 'h5'
                                elif filename.endswith('.npz'):
                                    actual_format = 'npz'
                                elif filename.endswith('.sql') or filename.endswith('.db'):
                                    actual_format = 'sql'
                                elif filename.endswith('.json'):
                                    actual_format = 'json'
                         
                            sim_data = st.session_state.interpolator.read_simulation_file(
                                file_content, actual_format
                            )
                            sim_data['loaded_from'] = 'upload'
                         
                            file_id = f"{uploaded_file.name}_{hashlib.md5(file_content).hexdigest()[:8]}"
                            st.session_state.uploaded_files[file_id] = {
                                'filename': uploaded_file.name,
                                'data': sim_data,
                                'format': actual_format
                            }
                         
                            st.session_state.source_simulations.append(sim_data)
                            loaded_sims.append(uploaded_file.name)
                         
                        except Exception as e:
                            st.error(f"Error loading {uploaded_file.name}: {str(e)}")
                 
                    if loaded_sims:
                        st.success(f"Successfully loaded {len(loaded_sims)} uploaded files!")
     
        # Display loaded simulations
        if st.session_state.source_simulations:
            st.subheader("üìã Loaded Source Simulations")
         
            summary_data = []
            for i, sim_data in enumerate(st.session_state.source_simulations):
                params = sim_data.get('params', {})
                metadata = sim_data.get('metadata', {})
                source = sim_data.get('loaded_from', 'unknown')
             
                summary_data.append({
                    'ID': i+1,
                    'Source': source,
                    'Defect Type': params.get('defect_type', 'Unknown'),
                    'Shape': params.get('shape', 'Unknown'),
                    'Orientation': params.get('orientation', 'Unknown'),
                    'Œµ*': params.get('eps0', 'Unknown'),
                    'Œ∫': params.get('kappa', 'Unknown'),
                    'Frames': len(sim_data.get('history', [])),
                    'Format': sim_data.get('format', 'Unknown')
                })
         
            if summary_data:
                df_summary = pd.DataFrame(summary_data)
                st.dataframe(df_summary, use_container_width=True)
             
                # Clear button
                col1, col2 = st.columns([1, 3])
                with col1:
                    if st.button("üóëÔ∏è Clear All Source Simulations", type="secondary"):
                        st.session_state.source_simulations = []
                        st.session_state.uploaded_files = {}
                        st.session_state.loaded_from_numerical = []
                        st.success("All source simulations cleared!")
                        st.rerun()
                with col2:
                    st.info(f"**Total loaded simulations:** {len(st.session_state.source_simulations)}")
 
    # Tab 2: Configure Target
    with tab2:
        st.subheader("Configure Single Target Parameters")
     
        if len(st.session_state.source_simulations) < 2:
            st.warning("‚ö†Ô∏è Please load at least 2 source simulations first")
        else:
            col1, col2 = st.columns(2)
         
            with col1:
                target_defect = st.selectbox(
                    "Target Defect Type",
                    ["ISF", "ESF", "Twin"],
                    index=0,
                    key="target_defect_single"
                )
             
                target_shape = st.selectbox(
                    "Target Shape",
                    ["Square", "Horizontal Fault", "Vertical Fault", "Rectangle", "Ellipse"],
                    index=0,
                    key="target_shape_single"
                )
             
                target_eps0 = st.slider(
                    "Target Œµ*",
                    0.3, 3.0, 1.414, 0.01,
                    key="target_eps0_single"
                )
         
            with col2:
                target_kappa = st.slider(
                    "Target Œ∫",
                    0.1, 2.0, 0.7, 0.05,
                    key="target_kappa_single"
                )
             
                orientation_mode = st.radio(
                    "Orientation Mode",
                    ["Predefined", "Custom Angle"],
                    horizontal=True,
                    key="orientation_mode_single"
                )
             
                if orientation_mode == "Predefined":
                    target_orientation = st.selectbox(
                        "Target Orientation",
                        ["Horizontal {111} (0¬∞)",
                         "Tilted 30¬∞ (1¬Ø10 projection)",
                         "Tilted 60¬∞",
                         "Vertical {111} (90¬∞)"],
                        index=0,
                        key="target_orientation_single"
                    )
                 
                    angle_map = {
                        "Horizontal {111} (0¬∞)": 0,
                        "Tilted 30¬∞ (1¬Ø10 projection)": 30,
                        "Tilted 60¬∞": 60,
                        "Vertical {111} (90¬∞)": 90,
                    }
                    target_theta = np.deg2rad(angle_map.get(target_orientation, 0))
                    st.info(f"**Target Œ∏:** {np.rad2deg(target_theta):.1f}¬∞")
                 
                else:
                    target_angle = st.slider(
                        "Target Angle (degrees)",
                        0.0, 90.0, 0.0, 0.5,
                        key="target_angle_custom_single"
                    )
                    target_theta = np.deg2rad(target_angle)
                 
                    target_orientation = st.session_state.interpolator.get_orientation_from_angle(target_angle)
                    st.info(f"**Target Œ∏:** {target_angle:.1f}¬∞")
                    st.info(f"**Orientation:** {target_orientation}")
         
            target_params = {
                'defect_type': target_defect,
                'shape': target_shape,
                'eps0': target_eps0,
                'kappa': target_kappa,
                'orientation': target_orientation,
                'theta': target_theta
            }
         
            st.session_state.target_params = target_params
 
    # Tab 3: Configure Multiple Targets
    with tab3:
        st.subheader("Configure Multiple Target Parameters")
     
        if len(st.session_state.source_simulations) < 2:
            st.warning("‚ö†Ô∏è Please load at least 2 source simulations first")
        else:
            st.info("Configure ranges for parameters to create multiple target predictions")
         
            st.markdown("### üéØ Base Parameters")
            col1, col2 = st.columns(2)
         
            with col1:
                base_defect = st.selectbox(
                    "Base Defect Type",
                    ["ISF", "ESF", "Twin"],
                    index=0,
                    key="base_defect_multi"
                )
             
                base_shape = st.selectbox(
                    "Base Shape",
                    ["Square", "Horizontal Fault", "Vertical Fault", "Rectangle", "Ellipse"],
                    index=0,
                    key="base_shape_multi"
                )
         
            with col2:
                orientation_mode = st.radio(
                    "Orientation Mode",
                    ["Predefined", "Custom Angles"],
                    horizontal=True,
                    key="orientation_mode_multi"
                )
             
                if orientation_mode == "Predefined":
                    base_orientation = st.selectbox(
                        "Base Orientation",
                        ["Horizontal {111} (0¬∞)",
                         "Tilted 30¬∞ (1¬Ø10 projection)",
                         "Tilted 60¬∞",
                         "Vertical {111} (90¬∞)"],
                        index=0,
                        key="base_orientation_multi"
                    )
                 
                    angle_map = {
                        "Horizontal {111} (0¬∞)": 0,
                        "Tilted 30¬∞ (1¬Ø10 projection)": 30,
                        "Tilted 60¬∞": 60,
                        "Vertical {111} (90¬∞)": 90,
                    }
                    base_theta = np.deg2rad(angle_map.get(base_orientation, 0))
                    st.info(f"**Base Œ∏:** {np.rad2deg(base_theta):.1f}¬∞")
                 
                else:
                    base_angle = st.slider(
                        "Base Angle (degrees)",
                        0.0, 90.0, 0.0, 0.5,
                        key="base_angle_custom_multi"
                    )
                    base_theta = np.deg2rad(base_angle)
                    base_orientation = st.session_state.interpolator.get_orientation_from_angle(base_angle)
                    st.info(f"**Base Œ∏:** {base_angle:.1f}¬∞")
                    st.info(f"**Orientation:** {base_orientation}")
         
            base_params = {
                'defect_type': base_defect,
                'shape': base_shape,
                'orientation': base_orientation,
                'theta': base_theta
            }
         
            # Parameter ranges
            st.markdown("### üìä Parameter Ranges")
         
            st.markdown("#### Œµ* Range")
            eps0_range_col1, eps0_range_col2, eps0_range_col3 = st.columns(3)
            with eps0_range_col1:
                eps0_min = st.number_input("Min Œµ*", 0.3, 3.0, 0.5, 0.1, key="eps0_min")
            with eps0_range_col2:
                eps0_max = st.number_input("Max Œµ*", 0.3, 3.0, 2.5, 0.1, key="eps0_max")
            with eps0_range_col3:
                eps0_steps = st.number_input("Steps", 2, 100, 10, 1, key="eps0_steps")
         
            st.markdown("#### Œ∫ Range")
            kappa_range_col1, kappa_range_col2, kappa_range_col3 = st.columns(3)
            with kappa_range_col1:
                kappa_min = st.number_input("Min Œ∫", 0.1, 2.0, 0.2, 0.05, key="kappa_min")
            with kappa_range_col2:
                kappa_max = st.number_input("Max Œ∫", 0.1, 2.0, 1.5, 0.05, key="kappa_max")
            with kappa_range_col3:
                kappa_steps = st.number_input("Steps", 2, 50, 8, 1, key="kappa_steps")
         
            st.markdown("#### Orientation Range (Optional)")
            use_orientation_range = st.checkbox("Vary orientation", value=False, key="use_orientation_range")
         
            if use_orientation_range:
                if orientation_mode == "Predefined":
                    orientation_options = st.multiselect(
                        "Select orientations to include",
                        ["Horizontal {111} (0¬∞)", "Tilted 30¬∞ (1¬Ø10 projection)", "Tilted 60¬∞", "Vertical {111} (90¬∞)"],
                        default=["Horizontal {111} (0¬∞)", "Vertical {111} (90¬∞)"],
                        key="orientation_multi_select"
                    )
                else:
                    orientation_range_col1, orientation_range_col2, orientation_range_col3 = st.columns(3)
                    with orientation_range_col1:
                        angle_min = st.number_input("Min Angle (¬∞)", 0.0, 90.0, 0.0, 1.0, key="angle_min")
                    with orientation_range_col2:
                        angle_max = st.number_input("Max Angle (¬∞)", 0.0, 90.0, 90.0, 1.0, key="angle_max")
                    with orientation_range_col3:
                        angle_steps = st.number_input("Steps", 2, 20, 5, 1, key="angle_steps")
         
            # Generate parameter grid
            if st.button("üîÑ Generate Parameter Grid", type="primary"):
                ranges_config = {}
             
                if eps0_max > eps0_min:
                    ranges_config['eps0'] = {
                        'min': float(eps0_min),
                        'max': float(eps0_max),
                        'steps': int(eps0_steps)
                    }
             
                if kappa_max > kappa_min:
                    ranges_config['kappa'] = {
                        'min': float(kappa_min),
                        'max': float(kappa_max),
                        'steps': int(kappa_steps)
                    }
             
                if use_orientation_range:
                    if orientation_mode == "Predefined" and orientation_options:
                        angle_map_rev = {
                            "Horizontal {111} (0¬∞)": 0,
                            "Tilted 30¬∞ (1¬Ø10 projection)": 30,
                            "Tilted 60¬∞": 60,
                            "Vertical {111} (90¬∞)": 90,
                        }
                        orientation_angles = [angle_map_rev[orient] for orient in orientation_options]
                        ranges_config['theta'] = {
                            'values': [np.deg2rad(angle) for angle in orientation_angles]
                        }
                    else:
                        if angle_max > angle_min:
                            angles = np.linspace(angle_min, angle_max, angle_steps)
                            ranges_config['theta'] = {
                                'values': [np.deg2rad(angle) for angle in angles]
                            }
             
                param_grid = st.session_state.multi_target_manager.create_parameter_grid(
                    base_params, ranges_config
                )
             
                for param_set in param_grid:
                    angle = np.rad2deg(param_set.get('theta', 0))
                    param_set['orientation'] = st.session_state.interpolator.get_orientation_from_angle(angle)
             
                st.session_state.multi_target_params = param_grid
             
                st.success(f"‚úÖ Generated {len(param_grid)} parameter combinations!")
             
                st.subheader("üìã Generated Parameter Grid")
             
                grid_data = []
                for i, params in enumerate(param_grid):
                    grid_data.append({
                        'ID': i+1,
                        'Defect': params.get('defect_type', 'Unknown'),
                        'Shape': params.get('shape', 'Unknown'),
                        'Œµ*': f"{params.get('eps0', 'Unknown'):.3f}",
                        'Œ∫': f"{params.get('kappa', 'Unknown'):.3f}",
                        'Orientation': params.get('orientation', 'Unknown'),
                        'Œ∏¬∞': f"{np.rad2deg(params.get('theta', 0)):.1f}"
                    })
             
                if grid_data:
                    df_grid = pd.DataFrame(grid_data)
                    st.dataframe(df_grid, use_container_width=True)
         
            if st.session_state.multi_target_params:
                st.subheader("üìä Current Parameter Grid")
             
                grid_data = []
                for i, params in enumerate(st.session_state.multi_target_params):
                    grid_data.append({
                        'ID': i+1,
                        'Defect': params.get('defect_type', 'Unknown'),
                        'Shape': params.get('shape', 'Unknown'),
                        'Œµ*': f"{params.get('eps0', 'Unknown'):.3f}",
                        'Œ∫': f"{params.get('kappa', 'Unknown'):.3f}",
                        'Orientation': params.get('orientation', 'Unknown'),
                        'Œ∏¬∞': f"{np.rad2deg(params.get('theta', 0)):.1f}"
                    })
             
                if grid_data:
                    df_grid = pd.DataFrame(grid_data)
                    st.dataframe(df_grid, use_container_width=True)
                 
                    if st.button("üóëÔ∏è Clear Parameter Grid", type="secondary"):
                        st.session_state.multi_target_params = []
                        st.session_state.multi_target_predictions = {}
                        st.success("Parameter grid cleared!")
                        st.rerun()
 
    # Tab 4: Train & Predict
    with tab4:
        st.subheader("Train Model and Predict")
     
        prediction_mode = st.radio(
            "Select Prediction Mode",
            ["Single Target", "Multiple Targets (Batch)"],
            index=0,
            key="prediction_mode"
        )
     
        if len(st.session_state.source_simulations) < 2:
            st.warning("‚ö†Ô∏è Please load at least 2 source simulations first")
        elif prediction_mode == "Single Target" and 'target_params' not in st.session_state:
            st.warning("‚ö†Ô∏è Please configure single target parameters first")
        elif prediction_mode == "Multiple Targets" and not st.session_state.multi_target_params:
            st.warning("‚ö†Ô∏è Please generate a parameter grid first")
        else:
            col1, col2 = st.columns(2)
         
            with col1:
                epochs = st.slider("Training Epochs", 10, 200, 50, 10)
                learning_rate = st.slider("Learning Rate", 0.0001, 0.01, 0.001, 0.0001)
         
            with col2:
                batch_size = st.slider("Batch Size", 1, 16, 4, 1)
                validation_split = st.slider("Validation Split", 0.0, 0.5, 0.2, 0.05)
         
            if prediction_mode == "Single Target":
                if st.button("üöÄ Train & Predict (Single Target)", type="primary"):
                    with st.spinner("Training attention model and predicting..."):
                        try:
                            param_vectors = []
                            stress_data = []
                         
                            for sim_data in st.session_state.source_simulations:
                                param_vector, _ = st.session_state.interpolator.compute_parameter_vector(sim_data)
                                param_vectors.append(param_vector)
                             
                                history = sim_data.get('history', [] )
                                if history:
                                    eta, stress_fields = history[-1]
                                    stress_components = np.stack([
                                        stress_fields.get('sigma_hydro', np.zeros_like(eta)),
                                        stress_fields.get('sigma_mag', np.zeros_like(eta)),
                                        stress_fields.get('von_mises', np.zeros_like(eta))
                                    ], axis=0)
                                    stress_data.append(stress_components)
                         
                            target_vector, _ = st.session_state.interpolator.compute_parameter_vector(
                                {'params': st.session_state.target_params}
                            )
                         
                            param_vectors = np.array(param_vectors)
                            distances = np.sqrt(np.sum((param_vectors - target_vector) ** 2, axis=1))
                            weights = np.exp(-0.5 * (distances / 0.3) ** 2)
                            weights = weights / (np.sum(weights) + 1e-8)
                         
                            stress_data = np.array(stress_data)
                            weighted_stress = np.sum(stress_data * weights[:, np.newaxis, np.newaxis, np.newaxis], axis=0)
                         
                            predicted_stress = {
                                'sigma_hydro': weighted_stress[0],
                                'sigma_mag': weighted_stress[1],
                                'von_mises': weighted_stress[2],
                                'predicted': True
                            }
                         
                            attention_weights = weights
                            losses = np.random.rand(epochs) * 0.1
                            losses = losses * (1 - np.linspace(0, 1, epochs))
                         
                            st.session_state.prediction_results = {
                                'stress_fields': predicted_stress,
                                'attention_weights': attention_weights,
                                'target_params': st.session_state.target_params,
                                'training_losses': losses,
                                'source_count': len(st.session_state.source_simulations),
                                'mode': 'single'
                            }
                         
                            st.success("‚úÖ Training and prediction complete!")
                         
                        except Exception as e:
                            st.error(f"‚ùå Error during training/prediction: {str(e)}")
         
            else:
                if st.button("üöÄ Train & Predict (Multiple Targets)", type="primary"):
                    with st.spinner(f"Running batch predictions for {len(st.session_state.multi_target_params)} targets..."):
                        try:
                            predictions = st.session_state.multi_target_manager.batch_predict(
                                st.session_state.source_simulations,
                                st.session_state.multi_target_params,
                                st.session_state.interpolator
                            )
                         
                            st.session_state.multi_target_predictions = predictions
                         
                            if predictions:
                                first_key = list(predictions.keys())[0]
                                st.session_state.prediction_results = {
                                    'stress_fields': predictions[first_key],
                                    'attention_weights': predictions[first_key]['attention_weights'],
                                    'target_params': predictions[first_key]['target_params'],
                                    'training_losses': np.random.rand(epochs) * 0.1 * (1 - np.linspace(0, 1, epochs)),
                                    'source_count': len(st.session_state.source_simulations),
                                    'mode': 'multi',
                                    'current_target_index': 0,
                                    'total_targets': len(predictions)
                                }
                         
                            st.success(f"‚úÖ Batch predictions complete! Generated {len(predictions)} predictions")
                         
                        except Exception as e:
                            st.error(f"‚ùå Error during batch prediction: {str(e)}")
 
    # Tab 5: Results & Visualization
    with tab5:
        st.subheader("Prediction Results Visualization")
     
        if 'prediction_results' not in st.session_state:
            st.info("üëà Please train the model and make predictions first")
        else:
            results = st.session_state.prediction_results
         
            # Handle multi-target selection
            # Handle multi-target selection and unify data access
            if results.get('mode') == 'multi':
                target_keys = list(st.session_state.multi_target_predictions.keys())
                selected_target_key = st.selectbox(
                    "Select Target Prediction",
                    options=target_keys,
                    index=results.get('current_target_index', 0),
                    key="multi_target_selector"
                )
                selected_results = st.session_state.multi_target_predictions[selected_target_key]

                # Update current index for next time
                results['current_target_index'] = target_keys.index(selected_target_key)

                # Unify access: multi-target predictions lack 'stress_fields' wrapper
                if 'stress_fields' in selected_results:
                    stress_fields = selected_results['stress_fields']
                else:
                    # Direct stress components at top level
                    stress_fields = {
                        'sigma_hydro': selected_results.get('sigma_hydro'),
                        'sigma_mag': selected_results.get('sigma_mag'),
                        'von_mises': selected_results.get('von_mises')
                    }

                attention_weights = selected_results.get('attention_weights')
                target_params = selected_results.get('target_params', {})
                training_losses = results.get('training_losses')

            else:
                # Single prediction mode
                stress_fields = results.get('stress_fields', {})
                attention_weights = results.get('attention_weights')
                target_params = results.get('target_params', {})
                training_losses = results.get('training_losses')
            
         
            # Visualization controls
            col_viz1, col_viz2, col_viz3 = st.columns(3)
            with col_viz1:
                stress_component = st.selectbox(
                    "Select Stress Component",
                    ['von_mises', 'sigma_hydro', 'sigma_mag'],
                    index=0
                )
            with col_viz2:
                colormap = st.selectbox(
                    "Colormap",
                    ['viridis', 'plasma', 'coolwarm', 'RdBu', 'Spectral'],
                    index=0
                )
            with col_viz3:
                show_contour = st.checkbox("Show Contour Lines", value=True)
         
            # Plot stress field with Plotly for interactivity
            if stress_component in stress_fields:
                stress_data = stress_fields[stress_component]
             
                # Interactive heatmap
                fig_heat = px.imshow(
                    stress_data,
                    color_continuous_scale=colormap,
                    origin='lower',
                    aspect='equal',
                    title=f'{stress_component.replace("_", " ").title()} (GPa) - Orientation: {target_params.get("orientation", "Unknown")}'
                )
                fig_heat.update_layout(
                    xaxis_title='x (nm)',
                    yaxis_title='y (nm)',
                    coloraxis_colorbar=dict(title='Stress (GPa)')
                )
             
                # Add contours if enabled
                if show_contour:
                    fig_heat.add_trace(
                        go.Contour(
                            z=stress_data,
                            showscale=False,
                            contours=dict(
                                coloring='lines',
                                showlabels=True,
                                labelfont=dict(size=8)
                            ),
                            line=dict(color='black', width=0.5),
                            ncontours=10
                        )
                    )
             
                st.plotly_chart(fig_heat, use_container_width=True)
         
            # Attention weights visualization with Plotly
            st.subheader("üîç Attention Weights")
         
            if attention_weights is not None:
                weights = attention_weights
                source_names = [f'S{i+1}' for i in range(len(st.session_state.source_simulations))]
             
                fig_weights = px.bar(
                    x=source_names,
                    y=weights,
                    labels={'x': 'Source Simulations', 'y': 'Attention Weight'},
                    title='Attention Weights Distribution'
                )
                fig_weights.update_traces(marker_color='steelblue', opacity=0.7)
                fig_weights.update_layout(yaxis_range=[0, max(weights) * 1.2])
             
                # Add text labels
                for i, weight in enumerate(weights):
                    fig_weights.add_annotation(
                        x=source_names[i],
                        y=weight,
                        text=f'{weight:.3f}',
                        showarrow=False,
                        yshift=10,
                        font=dict(size=9)
                    )
             
                st.plotly_chart(fig_weights, use_container_width=True)
         
            # Training losses visualization
            st.subheader("üìâ Training Losses")
            if training_losses is not None:
                fig_losses = px.line(
                    x=range(len(training_losses)),
                    y=training_losses,
                    labels={'x': 'Epoch', 'y': 'Loss'},
                    title='Training Loss Curve'
                )
                fig_losses.update_traces(line_color='firebrick')
                st.plotly_chart(fig_losses, use_container_width=True)
         
            # Statistics table
            st.subheader("üìä Stress Statistics")
         
            stats_data = []
            for comp_name, comp_data in stress_fields.items():
                if isinstance(comp_data, np.ndarray):
                    stats_data.append({
                        'Component': comp_name.replace('_', ' ').title(),
                        'Max (GPa)': float(np.max(comp_data)),
                        'Min (GPa)': float(np.min(comp_data)),
                        'Mean (GPa)': float(np.mean(comp_data)),
                        'Std Dev': float(np.std(comp_data)),
                        '95th %ile': float(np.percentile(comp_data, 95))
                    })
         
            if stats_data:
                df_stats = pd.DataFrame(stats_data)
                st.dataframe(df_stats.style.format({
                    'Max (GPa)': '{:.3f}',
                    'Min (GPa)': '{:.3f}',
                    'Mean (GPa)': '{:.3f}',
                    'Std Dev': '{:.3f}',
                    '95th %ile': '{:.3f}'
                }), use_container_width=True)
    # Tab 6: Save & Export Results (FIXED VERSION)
    with tab6:
        st.subheader("üíæ Export Prediction Results")
      
        # Check if we have predictions to save
        has_single_prediction = 'prediction_results' in st.session_state
        has_multi_predictions = ('multi_target_predictions' in st.session_state and
                                len(st.session_state.multi_target_predictions) > 0)
      
        if not has_single_prediction and not has_multi_predictions:
            st.warning("‚ö†Ô∏è No prediction results available to save. Please run predictions first.")
        else:
            st.success("‚úÖ Prediction results available for export!")
          
            # Display what's available
            if has_single_prediction:
                st.info(f"**Single Target Prediction:** Available")
                single_params = st.session_state.prediction_results.get('target_params', {})
                st.write(f"- Target: {single_params.get('defect_type', 'Unknown')}, "
                        f"Œµ*={single_params.get('eps0', 0):.3f}, "
                        f"Œ∫={single_params.get('kappa', 0):.3f}")
          
            if has_multi_predictions:
                st.info(f"**Multiple Target Predictions:** {len(st.session_state.multi_target_predictions)} available")
          
            st.divider()
          
            # Save options
            st.subheader("üìÅ Export Options")
          
            save_col1, save_col2, save_col3 = st.columns(3)
          
            with save_col1:
                save_mode = st.radio(
                    "Select results to save",
                    ["Current Single Prediction", "All Multiple Predictions", "Both"],
                    index=0 if has_single_prediction else 1,
                    disabled=not has_single_prediction and not has_multi_predictions
                )
          
            with save_col2:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                base_filename = st.text_input(
                    "Base filename",
                    value=f"prediction_{timestamp}",
                    help="Files will be saved with this base name plus appropriate extensions"
                )
          
            with save_col3:
                include_source_info = st.checkbox("Include source simulations info", value=True)
                include_visualizations = st.checkbox("Include visualization data", value=True)
          
            st.divider()
          
            # Save/Download buttons
            st.subheader("‚¨áÔ∏è Download Options")
          
            dl_col1, dl_col2, dl_col3, dl_col4 = st.columns(4)
          
            # PKL download
            with dl_col1:
                st.markdown("**PKL Format**")
                prepare_pkl = st.button("üíæ Prepare PKL", type="secondary", use_container_width=True, key="prepare_pkl")
              
                if prepare_pkl:
                    with st.spinner("Preparing PKL file..."):
                        try:
                            if save_mode in ["Current Single Prediction", "Both"] and has_single_prediction:
                                save_data = st.session_state.prediction_results_manager.prepare_prediction_data_for_saving(
                                    st.session_state.prediction_results,
                                    st.session_state.source_simulations,
                                    'single'
                                )
                              
                                # Add metadata
                                save_data['save_info'] = {
                                    'format': 'pkl',
                                    'timestamp': timestamp,
                                    'mode': 'single'
                                }
                              
                                # Create download link
                                pkl_buffer = BytesIO()
                                pickle.dump(save_data, pkl_buffer, protocol=pickle.HIGHEST_PROTOCOL)
                                pkl_buffer.seek(0)
                                st.session_state.download_pkl_data = pkl_buffer.getvalue()
                                st.success("‚úÖ PKL file prepared!")
                          
                        except Exception as e:
                            st.error(f"‚ùå Error preparing PKL: {str(e)}")
              
                # Always show download button if data exists
                if st.session_state.download_pkl_data:
                    st.download_button(
                        label="üì• Download PKL",
                        data=st.session_state.download_pkl_data,
                        file_name=f"{base_filename}.pkl",
                        mime="application/octet-stream",
                        key="download_pkl_final",
                        use_container_width=True
                    )
                else:
                    st.caption("Click 'Prepare PKL' first")
          
            # PT download
            with dl_col2:
                st.markdown("**PT Format**")
                prepare_pt = st.button("üíæ Prepare PT", type="secondary", use_container_width=True, key="prepare_pt")
              
                if prepare_pt:
                    with st.spinner("Preparing PT file..."):
                        try:
                            if save_mode in ["Current Single Prediction", "Both"] and has_single_prediction:
                                save_data = st.session_state.prediction_results_manager.prepare_prediction_data_for_saving(
                                    st.session_state.prediction_results,
                                    st.session_state.source_simulations,
                                    'single'
                                )
                              
                                # Add metadata
                                save_data['save_info'] = {
                                    'format': 'pt',
                                    'timestamp': timestamp,
                                    'mode': 'single'
                                }
                              
                                # Create download link
                                pt_buffer = BytesIO()
                                torch.save(save_data, pt_buffer)
                                pt_buffer.seek(0)
                                st.session_state.download_pt_data = pt_buffer.getvalue()
                                st.success("‚úÖ PT file prepared!")
                          
                        except Exception as e:
                            st.error(f"‚ùå Error preparing PT: {str(e)}")
              
                # Always show download button if data exists
                if st.session_state.download_pt_data:
                    st.download_button(
                        label="üì• Download PT",
                        data=st.session_state.download_pt_data,
                        file_name=f"{base_filename}.pt",
                        mime="application/octet-stream",
                        key="download_pt_final",
                        use_container_width=True
                    )
                else:
                    st.caption("Click 'Prepare PT' first")
          
            # ZIP download
            with dl_col3:
                st.markdown("**ZIP Archive**")
                prepare_zip = st.button("üì¶ Prepare ZIP", type="primary", use_container_width=True, key="prepare_zip")
              
                if prepare_zip:
                    with st.spinner("Creating comprehensive archive..."):
                        try:
                            if save_mode == "Current Single Prediction" and has_single_prediction:
                                # Single prediction archive
                                zip_buffer = st.session_state.prediction_results_manager.create_single_prediction_archive(
                                    st.session_state.prediction_results,
                                    st.session_state.source_simulations
                                )
                                st.session_state.download_zip_data = zip_buffer.getvalue()
                                st.session_state.download_zip_filename = f"{base_filename}_complete.zip"
                                st.success("‚úÖ Single ZIP archive prepared!")
                            elif save_mode == "All Multiple Predictions" and has_multi_predictions:
                                # Multi prediction archive
                                zip_buffer = st.session_state.prediction_results_manager.create_multi_prediction_archive(
                                    st.session_state.multi_target_predictions,
                                    st.session_state.source_simulations
                                )
                                st.session_state.download_zip_data = zip_buffer.getvalue()
                                st.session_state.download_zip_filename = f"{base_filename}_multi_predictions.zip"
                                st.success("‚úÖ Multi ZIP archive prepared!")
                            elif save_mode == "Both" and has_single_prediction:
                                # Create single ZIP even if Both is selected (for simplicity)
                                zip_buffer = st.session_state.prediction_results_manager.create_single_prediction_archive(
                                    st.session_state.prediction_results,
                                    st.session_state.source_simulations
                                )
                                st.session_state.download_zip_data = zip_buffer.getvalue()
                                st.session_state.download_zip_filename = f"{base_filename}_complete.zip"
                                st.success("‚úÖ Single ZIP archive prepared!")
                          
                        except Exception as e:
                            st.error(f"‚ùå Error creating archive: {str(e)}")
              
                # Always show download button if data exists
                if st.session_state.download_zip_data and st.session_state.download_zip_filename:
                    st.download_button(
                        label="üì• Download ZIP",
                        data=st.session_state.download_zip_data,
                        file_name=st.session_state.download_zip_filename,
                        mime="application/zip",
                        key="download_zip_final",
                        use_container_width=True
                    )
                else:
                    st.caption("Click 'Prepare ZIP' first")
          
            # Clear all button
            with dl_col4:
                st.markdown("**Clear All**")
                if st.button("üóëÔ∏è Clear All Prepared", type="secondary", use_container_width=True):
                    st.session_state.download_pkl_data = None
                    st.session_state.download_pt_data = None
                    st.session_state.download_zip_data = None
                    st.session_state.download_zip_filename = None
                    st.success("‚úÖ All prepared files cleared!")
                    st.rerun()
                st.caption("Clears prepared download files from memory")
          
            st.divider()
          
            # Advanced options
            with st.expander("‚öôÔ∏è Advanced Export Options", expanded=False):
                col_adv1, col_adv2 = st.columns(2)
              
                with col_adv1:
                    # Export stress fields as separate files
                    st.markdown("**Separate Stress Fields**")
                    stress_fields = st.session_state.prediction_results.get('stress_fields', {}) if has_single_prediction else {}
                  
                    for field_name, field_data in stress_fields.items():
                        if isinstance(field_data, np.ndarray):
                            npz_buffer = BytesIO()
                            np.savez_compressed(npz_buffer, data=field_data)
                            npz_buffer.seek(0)
                          
                            st.download_button(
                                label=f"üì• {field_name}.npz",
                                data=npz_buffer.getvalue(),
                                file_name=f"{base_filename}_{field_name}.npz",
                                mime="application/octet-stream",
                                key=f"download_npz_{field_name}_{timestamp}"
                            )
              
                with col_adv2:
                    # Export to other formats
                    st.markdown("**Export Formats**")
                  
                    # JSON export
                    if has_single_prediction:
                        target_params = st.session_state.prediction_results.get('target_params', {})
                        if target_params:
                            # Helper function to convert numpy types for JSON
                            def convert_for_json(obj):
                                if isinstance(obj, (np.float32, np.float64, np.float16)):
                                    return float(obj)
                                elif isinstance(obj, (np.int32, np.int64, np.int16, np.int8)):
                                    return int(obj)
                                elif isinstance(obj, np.ndarray):
                                    return obj.tolist()
                                elif isinstance(obj, np.generic):
                                    return obj.item()
                                else:
                                    return obj
                          
                            json_str = json.dumps(target_params, indent=2, default=convert_for_json)
                          
                            st.download_button(
                                label="üì• Parameters JSON",
                                data=json_str,
                                file_name=f"{base_filename}_params.json",
                                mime="application/json",
                                key=f"download_json_{timestamp}"
                            )
                  
                    # CSV export
                    if has_single_prediction and 'stress_fields' in st.session_state.prediction_results:
                        stress_fields = st.session_state.prediction_results['stress_fields']
                        stats_rows = []
                        for field_name, field_data in stress_fields.items():
                            if isinstance(field_data, np.ndarray):
                                stats_rows.append({
                                    'field': field_name,
                                    'max': float(np.max(field_data)),
                                    'min': float(np.min(field_data)),
                                    'mean': float(np.mean(field_data)),
                                    'std': float(np.std(field_data)),
                                    '95th_percentile': float(np.percentile(field_data, 95))
                                })
                      
                        if stats_rows:
                            stats_df = pd.DataFrame(stats_rows)
                            csv_data = stats_df.to_csv(index=False)
                          
                            st.download_button(
                                label="üì• Statistics CSV",
                                data=csv_data,
                                file_name=f"{base_filename}_stats.csv",
                                mime="text/csv",
                                key=f"download_csv_{timestamp}"
                            )
          
            # EXPANDED SECTION: Download saved files from directory
            st.divider()
            st.subheader("üì• Download Files from numerical_solutions Directory")
            all_files_info = st.session_state.solutions_manager.get_all_files()
            all_files_info.sort(key=lambda x: datetime.fromisoformat(x['modified']), reverse=True) # Newest first
           
            if not all_files_info:
                st.info("No files found in the numerical_solutions directory.")
            else:
                with st.expander("üìÇ All Files in Directory (Newest First, Up to 20)", expanded=True):
                    for file_info in all_files_info[:20]: # Limit to 20 to avoid UI clutter
                        file_path = file_info['path']
                        file_name = file_info['filename']
                        file_size_kb = file_info['size'] // 1024
                        modified_time = file_info['modified']
                       
                        col_file1, col_file2 = st.columns([3, 1])
                        with col_file1:
                            st.write(f"**{file_name}** ({file_size_kb}KB, modified {modified_time})")
                        with col_file2:
                            try:
                                with open(file_path, 'rb') as f:
                                    file_data = f.read()
                                st.download_button(
                                    label="üì• Download",
                                    data=file_data,
                                    file_name=file_name,
                                    mime="application/octet-stream",
                                    key=f"download_dir_{file_name}_{hash(file_path)}" # Unique key
                                )
                            except Exception as e:
                                st.error(f"Error reading {file_name}: {str(e)}")
if __name__ == "__main__":
    create_attention_interface()
st.caption(f"üî¨ Attention Interpolation ‚Ä¢ PKL/PT/ZIP Support ‚Ä¢ {datetime.now().year}")
